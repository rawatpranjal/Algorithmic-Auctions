{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e052c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of model\n",
    "gamma = 0.9\n",
    "u = 40\n",
    "v = 1\n",
    "w = 4\n",
    "n = 1\n",
    "sigma = 0.05\n",
    "rho = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1804f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(Q,u,v):\n",
    "    return u-v*Q\n",
    "\n",
    "def profit(P,Q):\n",
    "    return\n",
    "           \n",
    "class cournot:\n",
    "    def __init__(self,u,v,w,n):\n",
    "        self.u=u\n",
    "        self.v=v\n",
    "        self.w=w\n",
    "        self.n=n\n",
    "        self.Qw=(u-w)/v\n",
    "        self.Qc=n*(u-w)/v/(n+1)\n",
    "        self.Qm=(u-w)/2*v\n",
    "        self.Pw = price(self.Qw,u,v)\n",
    "        self.Pc = price(self.Qc,u,v)\n",
    "        self.Pm = price(self.Qm,u,v)\n",
    "        self.Πw = 0\n",
    "        self.Πc = (self.Pc-w)*self.Qc\n",
    "        self.Πm = (self.Pm-w)*self.Qm\n",
    "        self.qw = self.Qw/n\n",
    "        self.qc = self.Qc/n\n",
    "        self.qm = self.Qm/n\n",
    "        self.πw = 0\n",
    "        self.πc = self.Πc/n\n",
    "        self.πm = self.Πm/n\n",
    "    \n",
    "    def price(self,Q):\n",
    "        return self.u-v*(Q)\n",
    "    \n",
    "    def profit(self,Q):\n",
    "        return (self.price(Q)-w)*Q\n",
    "    \n",
    "def whichidx(value, array):\n",
    "    return np.argmin(np.abs(array-value))\n",
    "\n",
    "game1 = cournot(u,v,w,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1d0798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.0, 18.0, 22.0, 324.0, 324.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Competition\n",
    "game1.Qc, game1.qc, game1.Pc, game1.Πc, game1.profit(game1.qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import pygame\n",
    "\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "env.reset(seed=543)\n",
    "torch.manual_seed(543)\n",
    "gamma = 0.9\n",
    "log_interval = 1\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
