{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a9b733",
   "metadata": {},
   "source": [
    "## Cournot Monopoly with Dynamic Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412d457",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3829f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d30fba",
   "metadata": {},
   "source": [
    "### Cournot\n",
    "\n",
    "Dynamic Demand:\n",
    "- $p_t = u_t - v \\sum_i q_{it} + x_t$ \n",
    "- $x_t = \\rho x_{t-1} + e_t, e_t \\sim N(0,\\sigma)$\n",
    "\n",
    "Walrasian \n",
    "- $\\pi_w = \\max_{q_i}{(p - w) q_i}$ \n",
    "- $w = u - v Q$\n",
    "- $Q_w = \\frac{u-w}{v}$\n",
    "- $P_w = w$\n",
    "- $q_w \\in [0,Q_w], \\pi_w = 0, \\Pi_w = 0$\n",
    "\n",
    "Nash\n",
    "- $\\pi_c = \\max_{q_i}{(p(Q) - w) q_i}$ \n",
    "- $p(Q) - w - v q_i = 0$\n",
    "- $n p(Q) = n w + v Q = n u - n v Q$\n",
    "- $Q_c = \\frac{n(u-w)}{v(n+1)}$\n",
    "- $P_c = u - v Q$\n",
    "- $q_c = Q_c/n$\n",
    "\n",
    "Monopoly\n",
    "- $\\pi_m = \\max_{Q}{(p(Q) - w) Q}$ \n",
    "- $p(Q)=w+vQ=u-vQ$ \n",
    "- $Q_m =\\frac{u-w}{2v}$ \n",
    "- $P_m =\\frac{u-w}{2v}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1bfb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of model\n",
    "gamma = 0.9\n",
    "u = 40\n",
    "v = 1\n",
    "w = 4\n",
    "n = 2\n",
    "sigma = 0.05\n",
    "rho = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b9781f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(Q,u,v):\n",
    "    return u-v*Q\n",
    "\n",
    "def profit(P,Q):\n",
    "    return\n",
    "           \n",
    "class cournot:\n",
    "    def __init__(self,u,v,w,n):\n",
    "        self.u=u\n",
    "        self.v=v\n",
    "        self.w=w\n",
    "        self.n=n\n",
    "        self.Qw=(u-w)/v\n",
    "        self.Qc=n*(u-w)/v/(n+1)\n",
    "        self.Qm=(u-w)/2*v\n",
    "        self.Pw = price(self.Qw,u,v)\n",
    "        self.Pc = price(self.Qc,u,v)\n",
    "        self.Pm = price(self.Qm,u,v)\n",
    "        self.Πw = 0\n",
    "        self.Πc = (self.Pc-w)*self.Qc\n",
    "        self.Πm = (self.Pm-w)*self.Qm\n",
    "        self.qw = self.Qw/n\n",
    "        self.qc = self.Qc/n\n",
    "        self.qm = self.Qm/n\n",
    "        self.πw = 0\n",
    "        self.πc = self.Πc/n\n",
    "        self.πm = self.Πm/n\n",
    "    \n",
    "    def price(self,Q):\n",
    "        return self.u-v*(Q)\n",
    "    \n",
    "    def profit(self,Q):\n",
    "        return (self.price(Q)-w)*Q\n",
    "        \n",
    "def whichidx(value, array):\n",
    "    return np.argmin(np.abs(array-value))\n",
    "\n",
    "game1 = cournot(40,1,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4edf36c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.0, 12.0, 16.0, 4, 288.0, 324.0, 324.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game1.Qc, game1.qc, game1.Pc, game1.w, game1.Πc, game1.Πm, game1.profit(game1.Qm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde3a92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.0, 9.0, 22.0, 4, 324.0, 243)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game1.Qm, game1.qm, game1.Pm, game1.w, game1.Πm, game1.profit(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d31cb",
   "metadata": {},
   "source": [
    "### Shocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d39686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: ρ, σ, K\n",
    "# Output: KxK Transition Matrix\n",
    "P = np.array([[0.5,0.5,0],\n",
    "              [0.1, 0.8, 0.1],\n",
    "              [0, 0.5, 0.5]])\n",
    "\n",
    "Z = np.array([-4, 0, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a8912",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ad6821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.666666666666668 16.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([16.  , 16.22, 16.45, 16.67, 16.9 , 17.12, 17.35, 17.57, 17.8 ,\n",
       "       18.02, 18.24, 18.47, 18.69, 18.92, 19.14, 19.37, 19.59, 19.82,\n",
       "       20.04, 20.27, 20.49, 20.71, 20.94, 21.16, 21.39, 21.61, 21.84,\n",
       "       22.06, 22.29, 22.51, 22.73, 22.96, 23.18, 23.41, 23.63, 23.86,\n",
       "       24.08, 24.31, 24.53, 24.76, 24.98, 25.2 , 25.43, 25.65, 25.88,\n",
       "       26.1 , 26.33, 26.55, 26.78, 27.  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# define the Boltzmann action selection function\n",
    "def boltzmann_action_selection(Q, state2, temperature):\n",
    "    # compute the modified probabilities of each action using the Boltzmann distribution\n",
    "    max_Q = np.max(Q[state2,:])\n",
    "    logits = (Q[state2,:] - max_Q) / temperature\n",
    "    exp_logits = np.exp(logits)\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    # select an action using the computed probabilities\n",
    "    action = np.random.choice(range(len(probs)), p=probs)\n",
    "    return action\n",
    "\n",
    "# initialize the state\n",
    "state1 = 0\n",
    "state2 = 0\n",
    "u = 0\n",
    "uidx = 0\n",
    "\n",
    "# initialize the Q-table\n",
    "game = game1\n",
    "num_states2 = 3\n",
    "num_actions = 50\n",
    "P = np.array([[0.5,0.5,0],\n",
    "              [0.1, 0.8, 0.1],\n",
    "              [0, 0.5, 0.5]])\n",
    "\n",
    "Z = np.array([-4, 0, 4])\n",
    "\n",
    "uidx = int(round(num_states2/2))\n",
    "\n",
    "# parameters of model\n",
    "gamma = 0.9\n",
    "u = 40\n",
    "v = 1\n",
    "w = 4\n",
    "n = 2\n",
    "sigma = 0.05\n",
    "rho = 0.1\n",
    "\n",
    "Q1 = np.random.uniform(cournot(u,v,w,n).Πc,cournot(u,v,w,n).Πm,(num_states2, num_actions))\n",
    "\n",
    "# histories\n",
    "q1_history = []\n",
    "u_history = []\n",
    "qc_history=[]\n",
    "qm_history=[]\n",
    "pc_history=[]\n",
    "pm_history=[]\n",
    "\n",
    "print(cournot(u+4,v,w,n).Qc,cournot(u-4,v,w,n).Qm)\n",
    "\n",
    "action2quantity = np.linspace(16,27,num_actions)\n",
    "action2quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc3b07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Boltzmann Q-learning parameters\n",
    "alpha = 0.1\n",
    "initial_temperature = 1.0\n",
    "temperature_decay = 0.99999\n",
    "min_temperature = 0.01\n",
    "temperature = initial_temperature\n",
    "initial_eps = 1.0\n",
    "eps_decay = 0.99999\n",
    "min_eps = 0.01\n",
    "eps = initial_eps\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c19e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0.92,1.00, 49.00, 243.00,  1.00, 1.00\n",
      "10000,1.91,1.00, 21.00, 399.49,  0.90, 0.90\n",
      "20000,-3.27,1.00, 24.00, 312.52,  0.82, 0.82\n",
      "30000,2.37,1.00, 8.00, 323.96,  0.74, 0.74\n",
      "40000,1.10,1.00, 48.00, 354.09,  0.67, 0.67\n",
      "50000,-3.06,1.00, 11.00, 397.66,  0.61, 0.61\n",
      "60000,1.16,1.00, 10.00, 323.94,  0.55, 0.55\n",
      "70000,2.95,0.00, 7.00, 323.82,  0.50, 0.50\n",
      "80000,0.15,1.00, 14.00, 322.69,  0.45, 0.45\n",
      "90000,0.97,1.00, 8.00, 323.96,  0.41, 0.41\n",
      "100000,-0.74,0.00, 12.00, 323.52,  0.37, 0.37\n",
      "110000,-0.92,1.00, 31.00, 299.41,  0.33, 0.33\n",
      "120000,-1.40,1.00, 40.00, 375.20,  0.30, 0.30\n",
      "130000,-1.66,1.00, 25.00, 310.95,  0.27, 0.27\n",
      "140000,-1.35,0.00, 17.00, 241.44,  0.25, 0.25\n",
      "150000,-0.20,0.00, 39.00, 179.35,  0.22, 0.22\n",
      "160000,4.26,1.00, 11.00, 323.78,  0.20, 0.20\n",
      "170000,0.52,1.00, 9.00, 324.00,  0.18, 0.18\n",
      "180000,3.27,1.00, 0.00, 256.00,  0.17, 0.17\n",
      "190000,-0.30,1.00, 8.00, 323.96,  0.15, 0.15\n",
      "200000,2.39,1.00, 44.00, 261.94,  0.14, 0.14\n",
      "210000,1.76,1.00, 3.00, 255.55,  0.12, 0.12\n",
      "220000,0.40,1.00, 9.00, 324.00,  0.11, 0.11\n",
      "230000,1.79,1.00, 9.00, 324.00,  0.10, 0.10\n",
      "240000,0.88,1.00, 11.00, 323.78,  0.09, 0.09\n"
     ]
    }
   ],
   "source": [
    "# run Boltzmann Q-learning for a fixed number of episodes\n",
    "num_episodes = 1000000\n",
    "for episode in range(num_episodes):\n",
    "    if np.random.uniform()>eps:\n",
    "        # select an action using Boltzmann action selection\n",
    "        action1 = boltzmann_action_selection(Q1, uidx, temperature)\n",
    "    else:\n",
    "        action1 = np.random.choice(range(num_actions))\n",
    "\n",
    "    quantity1 = action2quantity[action1]\n",
    "\n",
    "    # rewards\n",
    "    reward1 = cournot(u,v,w,n).profit(quantity1)\n",
    "\n",
    "    # update the Q-value for the selected action in the current state\n",
    "    Q1[uidx][action1] =  (1-alpha)*Q1[uidx][action1] + alpha * (reward1 + gamma * np.max(np.average(Q1,axis=0,weights=P[uidx])))\n",
    "    \n",
    "    # generate shock\n",
    "    uidx = whichidx(np.random.choice(Z,p=P[uidx]), Z)\n",
    "    u = 40 + Z[uidx]\n",
    "    \n",
    "    # update temperature\n",
    "    temperature = max(min_temperature, temperature * temperature_decay)\n",
    "    eps = max(min_eps, eps * eps_decay)\n",
    "    error = reward1 + gamma * np.max(Q1[uidx])-Q1[uidx][action1] \n",
    "    errors.append(error)\n",
    "    if episode%10000 == 0:  \n",
    "        print(f'{episode},{np.mean(errors[-10000:]):0.2f},{uidx:0.2f}, {action1:0.2f}, {reward1:0.2f},  {temperature:0.2f}, {eps:0.2f}')\n",
    "    \n",
    "    u_history.append(u)\n",
    "    q1_history.append(quantity1)\n",
    "    qc_history.append(cournot(u,v,w,n).Qc)\n",
    "    qm_history.append(cournot(u,v,w,n).Qm)\n",
    "    pc_history.append(cournot(u,v,w,n).profit(quantity1))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc79cc",
   "metadata": {},
   "source": [
    "### Check Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922096cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uidx in range(num_states2):\n",
    "    action1 = boltzmann_action_selection(Q1, uidx, 0.0001)\n",
    "    print(uidx, action1, np.abs((1-alpha)*Q1[uidx][action1] + alpha * (reward1 + gamma * np.max(np.average(Q1,axis=0,weights=P[uidx])))-Q1[uidx][action1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9a3b3",
   "metadata": {},
   "source": [
    "### Check Probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.001\n",
    "def boltzmann_action_probs(Q, state2, temperature):\n",
    "    # compute the modified probabilities of each action using the Boltzmann distribution\n",
    "    max_Q = np.max(Q[state2,:])\n",
    "    logits = (Q[state2,:] - max_Q) / temperature\n",
    "    exp_logits = np.exp(logits)\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    # select an action using the computed probabilities\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Response\n",
    "for uidx in range(num_states2):\n",
    "        print(uidx,np.round(boltzmann_action_probs(Q1,uidx,temperature),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7886c03",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "k = 100\n",
    "plt.plot(u_history[-k:])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(qc_history[-k:])\n",
    "plt.plot(qm_history[-k:])\n",
    "plt.plot(q1_history[-k:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0c882",
   "metadata": {},
   "source": [
    "### Correlations with shocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "np.corrcoef(u_history[-k:], q1_history[-k:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c4160",
   "metadata": {},
   "source": [
    "### Impulse Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the state\n",
    "u = 40\n",
    "uidx = 0\n",
    "q1_impulse=[]\n",
    "qm_impulse = []\n",
    "qc_impulse = []\n",
    "temperature=0.0001\n",
    "# run Boltzmann Q-learning for a fixed number of episodes\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    if episode in [25]:\n",
    "        uidx = 1\n",
    "    elif episode in [50]:\n",
    "        uidx = 2\n",
    "    else:\n",
    "        action1 = boltzmann_action_selection(Q1,uidx,temperature)\n",
    "\n",
    "    quantity1 = action2quantity[action1]\n",
    "\n",
    "    # rewards\n",
    "    reward1 = cournot(u,v,w,n).profit(quantity1)\n",
    "\n",
    "   # generate shock\n",
    "    u = 40 + Z[uidx]\n",
    "    q1_impulse.append(quantity1)\n",
    "    qm_impulse.append(cournot(u,v,w,n).Qm)\n",
    "    qc_impulse.append(cournot(u,v,w,n).Qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fe09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(q1_impulse)\n",
    "plt.plot(qm_impulse, 'b', label = 'Monopoly Price')\n",
    "plt.plot(qc_impulse, 'r', label = 'Competition Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da855c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
