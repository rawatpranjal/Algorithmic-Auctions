{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "784407e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch\n",
    "import math\n",
    "import torch.optim as optim \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (30, 10)\n",
    "plt.rcParams['font.size']=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a429ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of model\n",
    "gamma = 0.9\n",
    "u = 40\n",
    "v = 1\n",
    "w = 4\n",
    "n = 1\n",
    "sigma = 0.05\n",
    "rho = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fec8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(Q,u,v):\n",
    "    return u-v*Q\n",
    "\n",
    "def profit(P,Q):\n",
    "    return\n",
    "           \n",
    "class cournot:\n",
    "    def __init__(self,u,v,w,n):\n",
    "        self.u=u\n",
    "        self.v=v\n",
    "        self.w=w\n",
    "        self.n=n\n",
    "        self.Qw=(u-w)/v\n",
    "        self.Qc=n*(u-w)/v/(n+1)\n",
    "        self.Qm=(u-w)/2*v\n",
    "        self.Pw = price(self.Qw,u,v)\n",
    "        self.Pc = price(self.Qc,u,v)\n",
    "        self.Pm = price(self.Qm,u,v)\n",
    "        self.Πw = 0\n",
    "        self.Πc = (self.Pc-w)*self.Qc\n",
    "        self.Πm = (self.Pm-w)*self.Qm\n",
    "        self.qw = self.Qw/n\n",
    "        self.qc = self.Qc/n\n",
    "        self.qm = self.Qm/n\n",
    "        self.πw = 0\n",
    "        self.πc = self.Πc/n\n",
    "        self.πm = self.Πm/n\n",
    "    \n",
    "    def price(self,Q):\n",
    "        return self.u-v*(Q)\n",
    "    \n",
    "    def profit(self,Q):\n",
    "        return (self.price(Q)-w)*Q\n",
    "    \n",
    "def whichidx(value, array):\n",
    "    return np.argmin(np.abs(array-value))\n",
    "\n",
    "game1 = cournot(u,v,w,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "995c8306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game1.profit(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af7dd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state','next_state','action','reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    # Holds transitions from experience and gives a random batch of transitions for training\n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self,*args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa33b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # Takes in a vector of states and gives out valuations for each action\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super(DQN,self).__init__()\n",
    "        self.layer1 = nn.Linear(n_obs, 128)\n",
    "        self.layer2 = nn.Linear(128,128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3900894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbs(agent, BETA):\n",
    "    return torch.exp(agent.policy_net(agent.state)/BETA)/torch.sum(torch.exp(agent.policy_net(agent.state)/BETA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0b1c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, N_STATES, N_ACTIONS, N_MEMORY=10000, BATCH_SIZE=512, GAMMA=0, TAU = 0.0001, LR = 1e-5,\n",
    "                 EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 1000, \n",
    "                 BETA_START = 0.1, BETA_END = 0.01, BETA_DECAY = 1000):\n",
    "        self.N_STATES = N_STATES\n",
    "        self.N_ACTIONS = N_ACTIONS\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.GAMMA = GAMMA\n",
    "        self.STEPS = 0\n",
    "        self.EPS_START = EPS_START\n",
    "        self.EPS_END = EPS_END\n",
    "        self.EPS_DECAY = EPS_DECAY\n",
    "        self.EPS_THRESHOLD = EPS_END + (EPS_START-EPS_END)*math.exp(-1.*self.STEPS/EPS_DECAY)\n",
    "        self.BETA_START = BETA_START\n",
    "        self.BETA_END = BETA_END\n",
    "        self.BETA_DECAY = BETA_DECAY\n",
    "        self.BETA_THRESHOLD = BETA_END + (BETA_START-BETA_END)*math.exp(-1.*self.STEPS/BETA_DECAY)\n",
    "        self.LR = LR\n",
    "        self.TAU = TAU\n",
    "        self.policy_net = DQN(N_STATES,N_ACTIONS)\n",
    "        self.target_net = DQN(N_STATES,N_ACTIONS)\n",
    "        self.state = torch.zeros(1,N_STATES)\n",
    "        self.actions = torch.tensor(np.arange(N_ACTIONS))\n",
    "        self.memory = ReplayMemory(N_MEMORY)\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(),lr=LR,amsgrad=True)\n",
    "        self.action_history = []\n",
    "        self.loss = torch.tensor(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d65fa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(agent):\n",
    "    # Given state selects action either from the DQN (Q values become probs) or randomly. \n",
    "    sample = random.random()\n",
    "    agent.EPS_THRESHOLD = agent.EPS_END + (agent.EPS_START-agent.EPS_END)*math.exp(-1.*agent.STEPS/agent.EPS_DECAY)\n",
    "    agent.BETA_THRESHOLD = agent.BETA_END + (agent.BETA_START-agent.BETA_END)*math.exp(-1.*agent.STEPS/agent.BETA_DECAY)\n",
    "    agent.STEPS += 1\n",
    "\n",
    "    if sample>agent.EPS_THRESHOLD:\n",
    "        with torch.no_grad():\n",
    "            probs = getProbs(agent, agent.BETA_THRESHOLD)     \n",
    "            index = probs.multinomial(num_samples=1, replacement=True)\n",
    "            choice = agent.actions[index]\n",
    "            return torch.tensor([[choice]],dtype=torch.long)\n",
    "    else:\n",
    "        randchoice = random.choice(np.arange(agent.N_ACTIONS))\n",
    "        return torch.tensor([[randchoice]],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78a3d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_net(agent):\n",
    "    target_net_state_dict = agent.target_net.state_dict()\n",
    "    policy_net_state_dict = agent.policy_net.state_dict()\n",
    "    for key in policy_net_state_dict:\n",
    "        target_net_state_dict[key] = policy_net_state_dict[key]*agent.TAU+target_net_state_dict[key]*(1-agent.TAU)\n",
    "    agent.target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7479d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(agent):\n",
    "    if len(agent.memory)<agent.BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Load data\n",
    "    transitions = agent.memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    \n",
    "    # final state is after simulation is done\n",
    "    # these are the \"s(t+1)\" from the transitions\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    # For these batch states we compute optimal policy according to our policy-network\n",
    "    # Compute Q(s,a) and get actions - these are the actions that would have been taken for each batch state \n",
    "    state_action_values = agent.policy_net(state_batch).gather(1,action_batch)\n",
    "    \n",
    "    # Compute V(s_t+1)\n",
    "    next_state_values = torch.zeros(agent.BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        # Use the \"old\" target_net to obtain value (Expected Return) on optimal actions from sampled s(t+1)\n",
    "        # This is Q_old(s',a')\n",
    "        next_state_values[non_final_mask] = agent.target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "    # Here we have add r + max Q_old(s',a')\n",
    "    expected_state_action_values = (next_state_values*agent.GAMMA)+reward_batch\n",
    "\n",
    "    # Compute the loss\n",
    "    # Q_new(s,a) - r + max Q_old(s',a'): is the error\n",
    "    # Huber loss function\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    agent.loss = criterion(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "    agent.optimizer.zero_grad()\n",
    "    agent.loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(agent.policy_net.parameters(),100)\n",
    "    agent.optimizer.step()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c709948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e1997b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(agent, q1c):\n",
    "    with torch.no_grad():\n",
    "        test = torch.tensor([[q1c]], dtype = torch.float32)\n",
    "        q = agent1.policy_net(test)\n",
    "        print(f'Loss:{agent.loss.item()},BETA:{round(agent.BETA_THRESHOLD,2)},EPS:{round(agent.EPS_THRESHOLD,2)},P:{getProbs(agent,agent.BETA_THRESHOLD)},Q:{q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c69b9af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.5 15.  16.5 18.  19.5 21.  22.5 24.  25.5 27. ] 18.0\n"
     ]
    }
   ],
   "source": [
    "N_ACTIONS = 10\n",
    "Qgrid = np.linspace(game1.qm*0.75, game1.qc*1.5, N_ACTIONS)\n",
    "print(Qgrid, game1.qm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ec4882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512 # number of transitions sampled from replay buffer\n",
    "GAMMA = 0.9 # Discount factor\n",
    "EPS_START = 0.9 # Initial value of Epsilon\n",
    "EPS_END = 0.01 # End value of Epsilon\n",
    "EPS_DECAY = 1000 # controls decay rate of Epsilon\n",
    "TAU = 0.005 # update rate of target network\n",
    "LR = 1e-4 # Learning rate of Adam\n",
    "BETA_START = 0.9 # temperature of the greedy-exploratory policy\n",
    "BETA_END = 0.1 # End value of temperature\n",
    "BETA_DECAY = 1000 # controls decay rate of temperature\n",
    "N_MEMORY = 10000\n",
    "N_STATES = 1\n",
    "\n",
    "agent1 = Agent(N_STATES, N_ACTIONS, N_MEMORY, BATCH_SIZE, GAMMA, TAU, LR,\n",
    "                    EPS_START, EPS_END, EPS_DECAY,\n",
    "                    BETA_START, BETA_END, BETA_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09344193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid multinomial distribution (encountering probability entry < 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Iteration:\u001b[39m\u001b[38;5;124m'\u001b[39m,t)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Take action\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m q1 \u001b[38;5;241m=\u001b[39m Qgrid[a1]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(q1)\n",
      "Cell \u001b[0;32mIn [26], line 11\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     probs \u001b[38;5;241m=\u001b[39m getProbs(agent, agent\u001b[38;5;241m.\u001b[39mBETA_THRESHOLD)     \n\u001b[0;32m---> 11\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     choice \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mactions[index]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([[choice]],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid multinomial distribution (encountering probability entry < 0)"
     ]
    }
   ],
   "source": [
    "prices = []\n",
    "quantities = []\n",
    "\n",
    "epochs = 10000\n",
    "for t in range(epochs):\n",
    "    print('\\n Iteration:',t)\n",
    "\n",
    "    # Take action\n",
    "    a1 = select_action(agent1)\n",
    "    q1 = Qgrid[a1]\n",
    "    \n",
    "    print(q1)\n",
    "    \n",
    "    # Obtain Reward\n",
    "    r1 = game1.profit(q1)\n",
    "    r1 = torch.tensor([r1])\n",
    "    \n",
    "    # Compute next state\n",
    "    next_state1 = agent1.state \n",
    "    \n",
    "    # store memory in transition\n",
    "    agent1.memory.push(agent1.state, next_state1, a1, r1)\n",
    "\n",
    "    # move to next state\n",
    "    agent1.state = next_state1\n",
    "    \n",
    "    # optimize\n",
    "    optimize_model(agent1)\n",
    "\n",
    "    # soft update target_net\n",
    "    update_target_net(agent1)\n",
    "    \n",
    "    # record actions\n",
    "    agent1.action_history.append(q1)\n",
    "    quantities.append(q1)\n",
    "    \n",
    "    # print loss\n",
    "    #track(agent1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41988f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., nan, 0., 0.]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProbs(agent1, agent1.BETA_THRESHOLD)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dba12b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1.policy_net(agent1.state)/agent1.BETA_THRESHOLD/torch.sum(torch.exp(agent1.policy_net(agent1.state)/agent1.BETA_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1da5e5a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid multinomial distribution (encountering probability entry < 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m q1 \u001b[38;5;241m=\u001b[39m Qgrid[a1]\n",
      "Cell \u001b[0;32mIn [26], line 11\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     probs \u001b[38;5;241m=\u001b[39m getProbs(agent, agent\u001b[38;5;241m.\u001b[39mBETA_THRESHOLD)     \n\u001b[0;32m---> 11\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     choice \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mactions[index]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([[choice]],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid multinomial distribution (encountering probability entry < 0)"
     ]
    }
   ],
   "source": [
    "    a1 = select_action(agent1)\n",
    "    q1 = Qgrid[a1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b909028e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.0, 18.0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1, game1.qm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f768e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
