{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: 34.00\tAverage reward: 11.20\n",
      "Episode 2\tLast reward: 20.00\tAverage reward: 11.64\n",
      "Episode 3\tLast reward: 12.00\tAverage reward: 11.66\n",
      "Episode 4\tLast reward: 10.00\tAverage reward: 11.58\n",
      "Episode 5\tLast reward: 40.00\tAverage reward: 13.00\n",
      "Episode 6\tLast reward: 11.00\tAverage reward: 12.90\n",
      "Episode 7\tLast reward: 10.00\tAverage reward: 12.75\n",
      "Episode 8\tLast reward: 14.00\tAverage reward: 12.81\n",
      "Episode 9\tLast reward: 21.00\tAverage reward: 13.22\n",
      "Episode 10\tLast reward: 29.00\tAverage reward: 14.01\n",
      "Episode 11\tLast reward: 20.00\tAverage reward: 14.31\n",
      "Episode 12\tLast reward: 13.00\tAverage reward: 14.25\n",
      "Episode 13\tLast reward: 24.00\tAverage reward: 14.73\n",
      "Episode 14\tLast reward: 18.00\tAverage reward: 14.90\n",
      "Episode 15\tLast reward: 10.00\tAverage reward: 14.65\n",
      "Episode 16\tLast reward: 9.00\tAverage reward: 14.37\n",
      "Episode 17\tLast reward: 54.00\tAverage reward: 16.35\n",
      "Episode 18\tLast reward: 12.00\tAverage reward: 16.13\n",
      "Episode 19\tLast reward: 20.00\tAverage reward: 16.33\n",
      "Episode 20\tLast reward: 23.00\tAverage reward: 16.66\n",
      "Episode 21\tLast reward: 60.00\tAverage reward: 18.83\n",
      "Episode 22\tLast reward: 16.00\tAverage reward: 18.69\n",
      "Episode 23\tLast reward: 31.00\tAverage reward: 19.30\n",
      "Episode 24\tLast reward: 67.00\tAverage reward: 21.69\n",
      "Episode 25\tLast reward: 11.00\tAverage reward: 21.15\n",
      "Episode 26\tLast reward: 41.00\tAverage reward: 22.14\n",
      "Episode 27\tLast reward: 59.00\tAverage reward: 23.99\n",
      "Episode 28\tLast reward: 28.00\tAverage reward: 24.19\n",
      "Episode 29\tLast reward: 17.00\tAverage reward: 23.83\n",
      "Episode 30\tLast reward: 23.00\tAverage reward: 23.79\n",
      "Episode 31\tLast reward: 30.00\tAverage reward: 24.10\n",
      "Episode 32\tLast reward: 32.00\tAverage reward: 24.49\n",
      "Episode 33\tLast reward: 23.00\tAverage reward: 24.42\n",
      "Episode 34\tLast reward: 31.00\tAverage reward: 24.75\n",
      "Episode 35\tLast reward: 30.00\tAverage reward: 25.01\n",
      "Episode 36\tLast reward: 52.00\tAverage reward: 26.36\n",
      "Episode 37\tLast reward: 59.00\tAverage reward: 27.99\n",
      "Episode 38\tLast reward: 40.00\tAverage reward: 28.59\n",
      "Episode 39\tLast reward: 31.00\tAverage reward: 28.71\n",
      "Episode 40\tLast reward: 28.00\tAverage reward: 28.68\n",
      "Episode 41\tLast reward: 23.00\tAverage reward: 28.39\n",
      "Episode 42\tLast reward: 26.00\tAverage reward: 28.27\n",
      "Episode 43\tLast reward: 125.00\tAverage reward: 33.11\n",
      "Episode 44\tLast reward: 101.00\tAverage reward: 36.50\n",
      "Episode 45\tLast reward: 40.00\tAverage reward: 36.68\n",
      "Episode 46\tLast reward: 68.00\tAverage reward: 38.25\n",
      "Episode 47\tLast reward: 132.00\tAverage reward: 42.93\n",
      "Episode 48\tLast reward: 133.00\tAverage reward: 47.44\n",
      "Episode 49\tLast reward: 141.00\tAverage reward: 52.11\n",
      "Episode 50\tLast reward: 23.00\tAverage reward: 50.66\n",
      "Episode 51\tLast reward: 129.00\tAverage reward: 54.58\n",
      "Episode 52\tLast reward: 123.00\tAverage reward: 58.00\n",
      "Episode 53\tLast reward: 107.00\tAverage reward: 60.45\n",
      "Episode 54\tLast reward: 132.00\tAverage reward: 64.02\n",
      "Episode 55\tLast reward: 106.00\tAverage reward: 66.12\n",
      "Episode 56\tLast reward: 123.00\tAverage reward: 68.97\n",
      "Episode 57\tLast reward: 83.00\tAverage reward: 69.67\n",
      "Episode 58\tLast reward: 169.00\tAverage reward: 74.64\n",
      "Episode 59\tLast reward: 106.00\tAverage reward: 76.20\n",
      "Episode 60\tLast reward: 143.00\tAverage reward: 79.54\n",
      "Episode 61\tLast reward: 40.00\tAverage reward: 77.57\n",
      "Episode 62\tLast reward: 191.00\tAverage reward: 83.24\n",
      "Episode 63\tLast reward: 153.00\tAverage reward: 86.73\n",
      "Episode 64\tLast reward: 106.00\tAverage reward: 87.69\n",
      "Episode 65\tLast reward: 139.00\tAverage reward: 90.26\n",
      "Episode 66\tLast reward: 68.00\tAverage reward: 89.14\n",
      "Episode 67\tLast reward: 104.00\tAverage reward: 89.89\n",
      "Episode 68\tLast reward: 122.00\tAverage reward: 91.49\n",
      "Episode 69\tLast reward: 135.00\tAverage reward: 93.67\n",
      "Episode 70\tLast reward: 153.00\tAverage reward: 96.63\n",
      "Episode 71\tLast reward: 170.00\tAverage reward: 100.30\n",
      "Episode 72\tLast reward: 134.00\tAverage reward: 101.99\n",
      "Episode 73\tLast reward: 51.00\tAverage reward: 99.44\n",
      "Episode 74\tLast reward: 187.00\tAverage reward: 103.82\n",
      "Episode 75\tLast reward: 121.00\tAverage reward: 104.67\n",
      "Episode 76\tLast reward: 224.00\tAverage reward: 110.64\n",
      "Episode 77\tLast reward: 193.00\tAverage reward: 114.76\n",
      "Episode 78\tLast reward: 138.00\tAverage reward: 115.92\n",
      "Episode 79\tLast reward: 159.00\tAverage reward: 118.07\n",
      "Episode 80\tLast reward: 153.00\tAverage reward: 119.82\n",
      "Episode 81\tLast reward: 179.00\tAverage reward: 122.78\n",
      "Episode 82\tLast reward: 121.00\tAverage reward: 122.69\n",
      "Episode 83\tLast reward: 87.00\tAverage reward: 120.91\n",
      "Episode 84\tLast reward: 130.00\tAverage reward: 121.36\n",
      "Episode 85\tLast reward: 128.00\tAverage reward: 121.69\n",
      "Episode 86\tLast reward: 132.00\tAverage reward: 122.21\n",
      "Episode 87\tLast reward: 160.00\tAverage reward: 124.10\n",
      "Episode 88\tLast reward: 171.00\tAverage reward: 126.44\n",
      "Episode 89\tLast reward: 168.00\tAverage reward: 128.52\n",
      "Episode 90\tLast reward: 219.00\tAverage reward: 133.04\n",
      "Episode 91\tLast reward: 218.00\tAverage reward: 137.29\n",
      "Episode 92\tLast reward: 313.00\tAverage reward: 146.08\n",
      "Episode 93\tLast reward: 249.00\tAverage reward: 151.22\n",
      "Episode 94\tLast reward: 343.00\tAverage reward: 160.81\n",
      "Episode 95\tLast reward: 486.00\tAverage reward: 177.07\n",
      "Episode 96\tLast reward: 293.00\tAverage reward: 182.87\n",
      "Episode 97\tLast reward: 163.00\tAverage reward: 181.88\n",
      "Episode 98\tLast reward: 302.00\tAverage reward: 187.88\n",
      "Episode 99\tLast reward: 355.00\tAverage reward: 196.24\n",
      "Episode 100\tLast reward: 392.00\tAverage reward: 206.03\n",
      "Episode 101\tLast reward: 359.00\tAverage reward: 213.67\n",
      "Episode 102\tLast reward: 249.00\tAverage reward: 215.44\n",
      "Episode 103\tLast reward: 297.00\tAverage reward: 219.52\n",
      "Episode 104\tLast reward: 237.00\tAverage reward: 220.39\n",
      "Episode 105\tLast reward: 478.00\tAverage reward: 233.27\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import pygame\n",
    "\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "env.reset(seed=543)\n",
    "torch.manual_seed(543)\n",
    "gamma = 0.9\n",
    "log_interval = 1\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1804f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.2.0-cp310-cp310-macosx_10_9_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d0798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
